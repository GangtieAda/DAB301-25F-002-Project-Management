{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Revisited\n",
    "\n",
    "Let's now revisit our MNIST. Knowing that the data contains 2-dimensional images of handwritten digits, we should be able to apply what we've learned about convolutions. Thus, in this section, we will create a convolutional neural network (CNN or convnet) for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to use a **validation set** to monitor our training progress. We can also use this validation set for *hyperparameter tuning*. Remember, using the validation set allows us to keep the *test set* to gauge how well our final model should do in the real world; that is, the final model only sees the test data once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first 10,000 samples of our training data as our validation set\n",
    "val_indices = list(range(10000))\n",
    "train_indices = list(range(10000, len(train_dataset)))\n",
    "\n",
    "# Create subset datasets\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "partial_train_dataset = Subset(train_dataset, train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in PyTorch, the data will have the shape `(batch_size, channels, height, width)` when passed through the network. PyTorch uses **channels first** convention by default. The MNIST images are automatically converted to this format by the `ToTensor()` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(partial_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the shape of a batch\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch shape: {images.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define our convolutional neural network using PyTorch's `nn.Module` class. `nn.Conv2d` creates the convolutional layers we have been discussing in the lectures. `nn.Flatten` is used to create a 1 dimensional vector so we can feed the output of our convolutional layers to the fully-connected layers. And `nn.Linear` is PyTorch's equivalent of Keras's Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Note: in_channels=1 for grayscale images\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, \n",
    "                               kernel_size=3, stride=1, padding='same')\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, \n",
    "                               kernel_size=3, stride=2, padding=\"valid\")  # padding='valid' is default\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, \n",
    "                               kernel_size=3, stride=1, padding='same')\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, \n",
    "                               kernel_size=3, stride=2, padding=\"valid\")\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)  # 64 channels, 6x6 spatial dimensions\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No softmax here - it's included in CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still tackling the same type of problem (multi-class classification) so the same loss and metrics will work for us here. PyTorch's `CrossEntropyLoss` combines softmax activation and negative log-likelihood loss. The optimizer `RMSprop` is the same as we used before and can be taken as the default method (or recipe) to try out for updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters())  #, lr=0.001) if you want to specify learning rate, default is 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train our model on the training data. For each batch of the training data, we compute the forward pass, calculate the loss, perform backpropagation, and update the parameters. After each *epoch* (going through all samples in our training data), we evaluate the model on the validation set. Note that the *validation data* is not being used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Store history\n",
    "    history['loss'].append(train_loss)\n",
    "    history['accuracy'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values for the training loss and accuracy, as well as the validation loss and accuracy, are stored in the `history` dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use this information to visualize the progress our network makes on the loss and accuracy as the number of epochs increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_values = history['loss']\n",
    "val_loss_values = history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# Code to plot the results\n",
    "plt.plot(epochs, loss_values, 'b', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss_values, 'r', label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, but this time we want to visualize the training and validation accuracy\n",
    "acc_values = history['accuracy']\n",
    "val_acc_values = history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'b', label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc_values, 'r', label=\"Validation Accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(epochs)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "For the model we have created, calculate the number of parameters by hand for each layer and compare to the output of the model summary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Recreate a similar model, except this time reduce the height and width by using pooling layers (e.g., `nn.MaxPool2d`) instead of convolution layers with stride 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
